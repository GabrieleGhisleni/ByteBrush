

<!-- toc -->

- [Tokenizer](#tokenizer)
  * [Special Tokens Configuration](#special-tokens-configuration)
  * [Model with no PAD token](#model-with-no-pad-token)
    + [Use [UNK] token](#use-unk-token)
    + [Set `pad_token` to `eos_token`](#set-pad_token-to-eos_token)
      - [Set the Attention Mask for the `eos_token`](#set-the-attention-mask-for-the-eos_token)
    + [Add a new token to the tokenizer](#add-a-new-token-to-the-tokenizer)
    + [`setup_chat` format](#setup_chat-format)
    + [Updating the Generation Configuration](#updating-the-generation-configuration)
  * [transformer implementation with [UNK] token](#transformer-implementation-with-unk-token)
  * [Troubleshooting](#troubleshooting)
    + [Padding Side](#padding-side)
    + [Generation never stops](#generation-never-stops)

<!-- tocstop -->

## Tokenizer

- https://github.com/huggingface/trl/blob/main/trl/models/utils.py#L40
- https://www.philschmid.de/fine-tune-llms-in-2024-with-trl
- https://huggingface.co/docs/transformers/llm_tutorial


### Special Tokens Configuration

1. **Special Tokens (`bos_token`, `eos_token`, `pad_token`)**: These tokens are crucial for controlling the flow of generation and handling variable-length inputs.
   - **`bos_token`** ("beginning of sequence token"): Signals the start of a text sequence. It's essential for the model to recognize where a new input begins.
   - **`eos_token`** ("end of sequence token"): Indicates the end of a text sequence. This is critical for the model to understand when to stop generating further text.
   - **`pad_token`**: Used to fill shorter sequences to match the length of the longest sequence in a batch for processing. It ensures consistent input sizes for the model's layers.

2. **Adding Special Tokens to the Tokenizer**: The tokenizer is updated to recognize these special tokens (`bos_token`, `eos_token`, and potentially others) as part of its vocabulary. This step is necessary so that the tokenizer can correctly encode and decode sequences that include these tokens. *Special tokens are called special because they are not derived from your input. They are added for a certain purpose and are independent of the specific input.* https://stackoverflow.com/questions/71679626/what-is-so-special-about-special-tokens

### Model with no PAD token
This is one of the most not clear part

#### Use [UNK] token
One way is to set the `pad_token` to the `unk_token` (unknown token) to avoid the model learning to predict the padding token. This is a common practice for language models, as the padding token is not part of the input sequence and should not be predicted. However, this approach might not be suitable for all use cases, especially when the `unk_token` is used to represent out-of-vocabulary words.

```python
# https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06
# https://github.com/unslothai/unsloth/blob/a030e802030d4619ba247d45c5819fb59e9addb3/unsloth/models/_utils.py#L119
def patch_tokenizer(model, tokenizer):
    if model is not None:
        model.config.update({"unsloth_version" : __version__})
    if not hasattr(tokenizer, "pad_token") or tokenizer.pad_token is None:
        # Fixes https://github.com/unslothai/unsloth/issues/5
        if hasattr(tokenizer, "unk_token"):
            tokenizer.add_special_tokens({"pad_token" : tokenizer.unk_token})
            tokenizer.pad_token = tokenizer.unk_token
        else:
            name = model.config._name_or_path if model is not None else "Model"
            logger.warning_one(
                f"{name} does not have a padding or unknown token!\n"\
                f"Will use the EOS token of id {tokenizer.eos_token_id} as padding."
            )
            assert(hasattr(tokenizer, "eos_token"))
            tokenizer.add_special_tokens({"pad_token" : tokenizer.eos_token})
            tokenizer.pad_token = tokenizer.eos_token
        if model is not None:
            model.config.update({"pad_token_id" : tokenizer.eos_token_id})
    pass
    return model, tokenizer
pass

```
#### Set `pad_token` to `eos_token`
An alternative approach is to set the `pad_token` to the `eos_token` and set the attention mask for the `eos_token` to 0. This ensures that the model learns to stop generating text when it encounters the padding token. This approach is more suitable for language models, as it allows the model to learn to predict the end of a sequence correctly.

This is because the loss on the padding token is not computed, and if it is equal to the `eos_token` the model won't learn to predict the `eos_token` correctly.
This approach is used in many tutorials and examples, but it's not clear if it's the best approach. In my experience, the model won't stop generating text when it encounters the padding token, and the results are not as expected.

```python
tokenizer = AutoTokenizer.from_pretrained(model_config.model_name_or_path, use_fast=True)

# https://github.com/huggingface/trl/blob/3b1911c2a99bc362992266a96743f67f3212218c/trl/trainer/sft_trainer.py#L216
tokenizer.pad_token = tokenizer.eos_token
```

##### Set the Attention Mask for the `eos_token`

```python
# Set the attention mask for the eos token to 0
model.config.eos_token_id = tokenizer.eos_token_id
model.config.pad_token_id = tokenizer.eos_token_id
```

then when creating the input for the model, you need to pass the `attention_mask` to indicate which tokens are padding tokens. This ensures that the model does not attend to the padding tokens during training and inference.

```python
# Create the input for the model
inputs = tokenizer(
    text,
    return_tensors="pt",
    padding="max_length",
    max_length=512,
    truncation=True,
    attention_mask=False
)
```

We set `attention_mask=False` to indicate that the padding tokens should not be attended to by the model. This is crucial for ensuring that the model does not generate text beyond the padding tokens.


#### Add a new token to the tokenizer

Last but not least, you can add a new token to the tokenizer and set it as the `pad_token`. This approach is useful when you want to use a specific token as the padding token, especially when the tokenizer does not have a built-in padding token. This method is more flexible and allows you to customize the padding token according to your specific use case.

```python
tokenizer.add_special_tokens({"pad_token": "<pad>"})
tokenizer.pad_token = "<pad>"
```

Then you need to resize the model's embedding layer to accommodate the new token. This is done to ensure that every token (old and new) has a corresponding vector in the model.

```python
model.resize_token_embeddings(len(tokenizer))
```

this is hard to understand, but it's the most complete and flexible way to do it. The embedding layer of the model maps token IDs to vectors of fixed size. When new tokens are added (like the special tokens in this case), the embedding layer needs to accommodate these additional tokens. Resizing the embedding layer ensures that every token (old and new) has a corresponding vector in the model. The resizing here is done to a multiple of 64, likely for optimization purposes related to how tensors are handled computationally, making the model more efficient.

The results from this are not clear, i was not able to finetune the model with this approach.

#### `setup_chat` format
If you do not start from a fine-tuned model, you can set the chat template to OAI chatML. This is a specific format for chat data that is used in the OpenAI ChatGPT model. This step is optional and can be skipped if you are starting from a fine-tuned model.

```python
from trl import setup_chat_format

# # set chat template to OAI chatML, remove if you start from a fine-tuned model
model, tokenizer = setup_chat_format(model, tokenizer)
```

we can see the implementation of it:
```python
# https://github.com/huggingface/trl/blob/a46cd84a6405312837f0d0e56fd1cf4d45585770/trl/models/utils.py#L43
def setup_chat_format(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    format: Optional[Literal["chatml"]] = "chatml",
    resize_to_multiple_of: Optional[int] = None,
) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:

    if format not in FORMAT_MAPPING:
        raise ValueError(f"Format {format} not available. Please use one of {FORMAT_MAPPING.keys()}")

    chat_format = FORMAT_MAPPING[format]()

    # set special tokens and them
    tokenizer.eos_token = chat_format.eos_token
    tokenizer.pad_token = chat_format.pad_token
    tokenizer.bos_token = chat_format.bos_token
    tokenizer.add_special_tokens({"additional_special_tokens": [chat_format.bos_token, chat_format.eos_token]})
    # set chat format for tokenizer
    tokenizer.chat_template = chat_format.chat_template

    # resize embedding layer to a multiple of 64, https://x.com/karpathy/status/1621578354024677377
    model.resize_token_embeddings(
        len(tokenizer), pad_to_multiple_of=resize_to_multiple_of if resize_to_multiple_of is not None else None
    )
    # Make sure to update the generation config to use the new eos & bos token
    if getattr(model, "generation_config", None) is not None:
        model.generation_config.bos_token_id = tokenizer.bos_token_id
        model.generation_config.eos_token_id = tokenizer.eos_token_id
        model.generation_config.pad_token_id = tokenizer.pad_token_id
```
#### Updating the Generation Configuration
The generation configuration of the model is updated to use the new `bos_token_id`, `eos_token_id`, and `pad_token_id`. This step is crucial for ensuring that the model's text generation behavior aligns with the intended use of the special tokens. For instance, knowing the correct `eos_token_id` allows the model to stop generating text appropriately.

```python
# Make sure to update the generation config to use the new eos & bos token
if getattr(model, "generation_config", None) is not None:
    model.generation_config.bos_token_id = tokenizer.bos_token_id
    model.generation_config.eos_token_id = tokenizer.eos_token_id
    model.generation_config.pad_token_id = tokenizer.pad_token_id

print(tokenizer.bos_token, tokenizer.decode(model.generation_config.bos_token_id))
print(tokenizer.eos_token, tokenizer.decode(model.generation_config.eos_token_id))
print(tokenizer.pad_token, tokenizer.decode(model.generation_config.pad_token_id))
```

### transformer implementation with [UNK] token

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=True,
    trust_remote_code=True,
    padding_side="right",
    #https://github.com/huggingface/transformers/issues/22794#issuecomment-1515452210
    add_eos_token=True,
  )

# https://github.com/unslothai/unsloth/blob/a030e802030d4619ba247d45c5819fb59e9addb3/unsloth/models/
if not hasattr(tokenizer, "pad_token") or tokenizer.pad_token is None:
  if hasattr(tokenizer, "unk_token"):
    tokenizer.add_special_tokens({"pad_token" : tokenizer.unk_token})
    tokenizer.pad_token = tokenizer.unk_token

  if model is not None:
    config = model.config.update({"pad_token_id" : tokenizer.eos_token_id})

print("unk", tokenizer.unk_token)
print("bos", tokenizer.bos_token, tokenizer.decode(model.generation_config.bos_token_id))
print("eos", tokenizer.eos_token, tokenizer.decode(model.generation_config.eos_token_id))
try: print("pad:", tokenizer.pad_token, tokenizer.decode(model.generation_config.pad_token_id))
except Exception: print("pad:", tokenizer.pad_token, model.generation_config.pad_token_id)
print("add_bos_token: ",tokenizer.add_bos_token, "add_eos_token: ", tokenizer.add_eos_token)
print(model.generation_config)
```

### Troubleshooting
#### Padding Side

- https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side

> LLMs are decoder-only architectures, meaning they continue to iterate on your input prompt. If your inputs do not have the same length, they need to be padded. Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded. Make sure you also don’t forget to pass the attention mask to generate!

- But all the code tutorials put `padding_side='right'`.
- trl raise a warning with `padding_side='left'`

> `UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.`

#### Generation never stops

- https://medium.com/@mayvic/solving-the-issue-of-falcon-text-generation-never-stopping-e8f599eae8f0
- https://github.com/huggingface/transformers/issues/22794#issuecomment-1515452210
- https://github.com/huggingface/transformers/issues/22794#issuecomment-1573966012

> I believe if you just set the `pad_token = eos_token` the model still is not learning to predict the eos_token because the corresponding attn_mask does not include the token and the labels ignores that token - i.e. no loss is computed for it.

- https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06
> I have seen some notebooks setting up the pad_token like: tokenizer.pad_token = tokenizer.eos_token > But I don’t recommend it, because eos_token (i.e., </s> for Mistral tokenizer) signals the LLM to stop generating tokens. If you set the eos_token as a padding token, the tokenizer will set the eos_token attention mask as “0”. The model will tend to ignore the eos_token and might over-generate tokens, which is not ideal for a down-streaming task. A more suitable option will be unk_token , because of its rarity, it will barely degrade the model’s performance even if we set its attention mask to “0” (i.e., set it as a padding token)

### Sources:
- https://mvschamanth.medium.com/decoder-only-transformer-model-521ce97e47e2
- https://arxiv.org/pdf/2312.05934.pdf
- https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
- https://github.com/openai/transformer-debugger
