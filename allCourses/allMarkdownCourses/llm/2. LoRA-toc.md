

<!-- toc -->

- [LoRA](#lora)
  * [QA from the article](#qa-from-the-article)
    + [QLoRA Compute-Memory Trade-offs](#qlora-compute-memory-trade-offs)
    + [Multiple Training epochs](#multiple-training-epochs)
    + [LoRA layers](#lora-layers)
    + [Balancing LoRA Hyperparameters: R and Alpha](#balancing-lora-hyperparameters-r-and-alpha)
    + [Does LoRA Work for Domain Adaptation?](#does-lora-work-for-domain-adaptation)
  * [LoRA Configuration](#lora-configuration)
  * [Transformer implementation](#transformer-implementation)

<!-- tocstop -->
# LoRA

LoRA (Low-Rank Adaptation) offers a parameter-efficient alternative. Here's the core idea:

1. Decomposing the Update Matrix: Instead of directly modifying the full weight matrix, LoRA decomposes it into two lower-rank matrices. These capture the essential updates for the task.
2. LoRA Adapter Modules: These lower-rank matrices form the "LoRA adapter" modules. They are much smaller than the original weight matrix, significantly reducing memory usage.
3. Efficient Training: During fine-tuning, only the parameters within the LoRA adapter modules are updated. This streamlines the process compared to full fine-tuning.

![Lora](https://assets-global.website-files.com/62c4a9809a85693c49c4674f/65b80a7f61892487cf1e3af6_lora-1.png)

Constraining Updates with Low-Rank Decomposition:

We denote the pre-trained weight matrix as $(W_0 \in \mathbb{R}^{d \times k})$, where $d$ and $k$ represent its dimensions. During fine-tuning, we constrain the update of \(W_0\) using a low-rank decomposition. This means we represent the update as the sum of $W_0$ and a lower-rank matrix $\Delta W$. The low-rank decomposition is achieved as $W_0 + \Delta W = W_0 + BA$, where:
- $B \in \mathbb{R}^{d \times r}$ is a matrix with dimensions $d$ (same as $(W_0)$'s rows) and $r$.
- $A \in \mathbb{R}^{r \times k}$ is a matrix with dimensions $r$ (rank of the decomposition) and $k$ (same as $W_0$'s columns).
- $r$ is the rank of the decomposition, typically chosen to be much smaller than both  $d$ and $k$.

## LoRA Rank 

While LoRA offers significant efficiency benefits, the choice of rank (number of columns/rows) in the LoRA adapter matrices can influence the trade-off between training speed and fine-tuning performance.

1. Low Ranks (e.g., 4-8):
   - Benefits: Extremely fast training and minimal memory footprint. 
   - Limitations: May not capture sufficient task-specific details because you are not trying to teach the model any new info or high concepts. Similar to the analogy of "smudging a dirty rag" on the original weight matrix, these low-rank adapters might introduce a general stylistic shift but lack the resolution to learn intricate task nuances. 
   - Suitability: Potentially appropriate for tasks emphasizing output format over precise content (e.g., text summarization style control).

2. Medium Ranks (e.g., 128-256):
   - Balance: Offer a reasonable compromise between training efficiency and capturing task-specific information. 
   - Analogy: Not directly mentioned, but these ranks can be thought of as using a finer "cleaning cloth" on the weight matrix, allowing for more detailed adjustments compared to very low ranks.

3. High Ranks (approaching full model dimensionality):
   - Benefits: Approach the performance of full fine-tuning. 
   - Drawbacks: Diminishing efficiency gains as the number of parameters in the adapter matrices increases.

## Lora Alpha
LoRA alpha controls the aggressiveness of the weight updates within the LoRA adapter modules, influencing how much they steer the model towards the target task.

Impact of LoRA Alpha:

1. Higher Alpha (α > rank):
   - Increased Influence: A higher alpha amplifies the impact of the LoRA adapter on the final model, potentially leading to faster learning of task-specific knowledge.
   - Potential Drawbacks:
     - Overshooting the optimal solution: Too high an alpha might cause the adapter to overpower the original model weights, leading to suboptimal performance.
     - Requires careful tuning: Finding the right balance with alpha can be crucial.
     
2. Lower Alpha (α == rank or less):
   - Conservative Updates: A lower alpha value leads to more conservative weight updates in the adapter modules, potentially requiring more training iterations for adaptation.
   - Safer Option: It can be a safer choice when starting out or if the task is particularly complex.

While there's not a formal equation solely for alpha, we can conceptually represent its impact as:

$$Updated Model Weights =  W_0 + \alpha * (\Delta A, \Delta B)$$

Here, α scales the weight updates ($\Delta A$ and $\Delta B$) before they are incorporated into the final model weights.

### LoRA layers

LoRA can be applied to selectively modify subsets of weight matrices in a neural network, effectively reducing the total number of parameters that need to be trained. 
Specifically, within the Transformer architecture, there are four key weight matrices in the self-attention module ($W_q, W_k, W_v, W_o$) and two in the MLP module. 
For the purpose of simplification and enhancing parameter efficiency, $W_q$ (as well as $W_k, W_v$) is considered as a unified matrix with dimensions $d_{\text{model}} \times d_{\text{model}}$, despite the fact that its output dimension is typically divided among multiple attention heads. The adaptation of attention weights for downstream tasks is the focus, while the MLP modules are kept constant (i.e., not trained for downstream tasks).

While LoRA (Low-Rank Adaptation) offers efficiency benefits, applying it to all layers in a large model has its considerations:

- Increased Trainable Parameters: Enabling LoRA for every layer multiplies the number of trainable parameters incrementally, potentially leading to a larger memory footprint.
- Higher Memory Needs: This translates to a larger memory footprint.
- Potential Performance Gains: However, the trade-off can be worthwhile. Enabling LoRA for all layers can lead to a noticeable improvement in modeling performance.

Applying LoRA to all layers offers a way to potentially enhance the model's capabilities, but it comes at the cost of increased training complexity due to the growth in parameters and memory requirements.

### transformer implementation

```python
# https://github.com/unslothai/unsloth/blob/a030e802030d4619ba247d45c5819fb59e9addb3/unsloth/models/llama.py#L1180
# - **`r`:** (int) — Lora attention dimension (the “rank”).
# - **`target_modules:** (Optional[Union[List[str], str]]) — The names of the modules to apply the adapter to. If this is specified, only the modules with the specified names will be replaced. When passing a string, a regex match will be performed. When passing a list of strings, either an exact match will be performed or it is checked if the name of the module ends with any of the passed strings. If this is specified as ‘all-linear’, then all linear/Conv1D modules are chosen, excluding the output layer. If this is not specified, modules will be chosen according to the model architecture. If the architecture is not known, an error will be raised — in this case, you should specify the target modules manually.
# - **`lora_alpha:** (int) — The alpha parameter for Lora scaling.
# - **`lora_dropout:** (float) — The dropout probability for Lora layers.
# - **`bias:** (str) — Bias type for LoRA. Can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. Be aware that this means that, even when disabling the adapters, the model will not produce the same output as the base model would have without adaptation.
# - **`use_rslora:** (bool) — When set to True, uses Rank-Stabilized LoRA which sets the adapter scaling factor to lora_alpha/math.sqrt(r), since it was proven to work better. Otherwise, it will use the original default value of lora_alpha/r.
# - **`init_lora_weights:** (bool | Literal["gaussian", "loftq"]) — How to initialize the weights of the adapter layers. Passing True (default) results in the default initialization from the reference implementation from Microsoft. Passing ‘gaussian’ results in Gaussian initialization scaled by the LoRA rank for linear and layers. Setting the initialization to False leads to completely random initialization and is discouraged. Pass 'loftq' to use LoftQ initialization. The initialization of LoRA weights is controlled by the parameter init_lora_weights of the LoraConfig. *By default, PEFT initializes LoRA weights the same way as the reference implementation, i.e. using Kaiming-uniform for weight A and initializing weight B as zeros, resulting in an identity transform.*
# - **`loftq_config:** (Optional[LoftQConfig]) — The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights and initialize Lora layers. Also pass init_lora_weights='loftq'. Note that you should not pass a quantized model in this case, as LoftQ will quantize the model itself.

from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training

# must check the modules of the model and finding the Linear layers
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

lora_alpha = 128
lora_r = 256

lora_config = LoraConfig(
    r=lora_r,
    lora_alpha=lora_alpha,
    target_modules=target_modules,
    lora_dropout=0,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    layers_to_transform=None,
    init_lora_weights=True,
    loftq_config={},
    use_rslora=False,
)
model = get_peft_model(model, lora_config)
model = prepare_model_for_kbit_training(
    model,
    use_gradient_checkpointing=True,
)
```

## Sources
- https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725
- https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms
- https://arxiv.org/pdf/2106.09685.pdf
- https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133

