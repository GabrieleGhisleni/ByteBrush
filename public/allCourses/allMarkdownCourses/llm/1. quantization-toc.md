

<!-- toc -->

- [Floating-Point Representations](#floating-point-representations)
  * [Implications for Machine Learning](#implications-for-machine-learning)
- [Introduction to model quantization](#introduction-to-model-quantization)
  * [8-bit vs 4-bit Quantization](#8-bit-vs-4-bit-quantization)
  * [Performance Impact](#performance-impact)
  * [Types of Quantization](#types-of-quantization)
      - [Zero-Point Quantization](#zero-point-quantization)
      - [Absmax Quantization](#absmax-quantization)
- [Transformers - bitsandbytes Integration](#transformers---bitsandbytes-integration)
  * [Explanation of Arguments](#explanation-of-arguments)

<!-- tocstop -->



Large Language Models (LLMs) and Machine Learning (ML) algorithms rely heavily on numerical computations. Floating-point representation is a method for encoding real numbers in computer memory. It's a compromise between accuracy and storage efficiency. Unlike integers, which represent whole numbers exactly, floating-point numbers can represent a wider range of values, including decimals and very large or small numbers. This flexibility is crucial for LLMs dealing with probabilities and complex relationships in text data, and for ML algorithms handling diverse datasets.

The size of a machine learning model is influenced by two key aspects:
1. Number of Parameters: Each model architecture has a specific number of parameters that define its capabilities. These parameters are adjusted during the training process and essentially capture the complex relationships within the data. A higher number of parameters generally leads to a more expressive model, but also results in a larger model size.
2. Precision of the Parameters: The data type used to store the parameters also plays a role in the model's size.

## Floating-Point Representations

![dtypes](https://blogs.nvidia.com/wp-content/uploads/2020/05/tf32-Mantissa-chart-hi-res-FINAL.png.webp)


- **FP32 (Float32)**
  - Description: The industry standard for representing floating-point numbers. It offers a balance between precision and speed, with a wide range of representable numbers and a decent computational speed.
  - Pros: Most precise format among the ones discussed, works well for tasks requiring high accuracy.
  - Cons: Slower computation speed due to its 32-bit size.
  - Supported GPUs: All GPUs support FP32.


- **FP16 (Float16)**
  - Description: A 16-bit format that prioritizes speed over precision. It can perform calculations much faster than FP32 but with a reduced range and precision.
  - Pros: Significantly faster computations compared to FP32.
  - Cons: Less precise, can lead to overflow (excessively large numbers) or underflow (very small numbers) errors in some cases. Not ideal for tasks requiring high accuracy.
  - Supported GPUs: Most modern GPUs support FP16. However, the specific capabilities for FP16 computations may vary depending on the GPU architecture.


- **BF16 (BFloat16)**
  - Description: A 16-bit format designed to strike a balance between FP16 and FP32. It offers a similar range to FP32 but with a slightly lower precision than FP16.
  - Pros: Maintains a good range of representable numbers like FP32 while offering faster computations than FP32. May be less prone to overflow/underflow errors compared to FP16 for certain tasks.
  - Cons: Not as widely supported as FP16 or FP32. Might have slightly reduced precision compared to FP32.
  -Supported GPUs: Generally supported on NVIDIA GPUs starting from the Turing architecture (e.g., RTX 20 series).

- **TF32 (TensorFloat-32)**
  - Description: A proprietary format introduced by NVIDIA for their Ampere architecture GPUs (e.g., RTX 30 series). It uses a combination of the exponent field from FP32 and the mantissa (significant digits) field from FP16.
  - Pros: Designed to offer a balance between the precision of FP16 and the range of BF16, potentially leading to faster computations with minimal accuracy loss.
  - Cons: Limited to NVIDIA Ampere architecture GPUs only. Not as widely supported as other formats.
  - Supported GPUs: NVIDIA GPUs with Ampere architecture (RTX 30 series and A100)

### Implications for ML

When fine-tuning large language models (LLMs) like those available through Hugging Face's Transformers library, the choice of floating-point representation can significantly impact both the training speed and the model's final performance.

- **Speed vs. Precision**: Using FP32 offers the highest precision but at the cost of computational speed. This can make training large models prohibitively slow. FP16 and BF16, being half-precision formats, offer a good compromise by speeding up the training process, though they introduce risks of numerical instability.
- **Mixed Precision Training**: To mitigate the downsides of FP16/BF16 (like overflow and underflow) while still benefiting from their speed, mixed precision training is often used. This approach involves performing computations in FP16/BF16 to leverage their speed, while maintaining a copy of the model weights in FP32 to preserve precision. Gradients computed in the lower precision format are converted back to FP32 for weight updates, ensuring that the model benefits from the speed of FP16/BF16 without significant loss in accuracy.
- **Hardware Support**: The choice of precision may also be influenced by the hardware. For instance, NVIDIA's GPUs have specialized tensor cores optimized for FP16/BF16 computations, significantly accelerating operations in these formats.

## Quantization

Quantization refers to the process of transforming data from a high-precision format (like FP32) to a lower-precision format. This essentially means converting a wider range of possible values into a more limited set of values.
8-bit quantization is a specific type of quantization where the data is compressed from its original format into a format using only 8 bits.

### 8-bit vs 4-bit Quantization
Quantization can be performed at different levels of precision, with 8-bit and 4-bit quantization being common choices for machine learning models.

**8-bit Quantization (`load_in_8bit`):**
- This reduces the precision of the model's weights and activations to 8 bits.
- It's a balance between model size, speed, and accuracy.
- Generally, 8-bit quantization is widely supported and considered a good trade-off for many applications.

**4-bit Quantization (`load_in_4bit`):**
- This further reduces the precision to 4 bits.
- It offers even smaller model sizes and faster inference times but can lead to a more significant drop in accuracy or performance compared to 8-bit.
- 4-bit quantization is more challenging to implement effectively due to the increased risk of losing important information during quantization.

#### Performance Impact
- **Decrease in Performance:** Moving from 8-bit to 4-bit quantization can indeed lead to a decrease in model performance (accuracy, precision, etc.), as the model has less information to work with. However, for some tasks, the impact might be minimal, making the trade-off for speed and size worthwhile.
- **Speed and Size:** 4-bit quantization can significantly reduce model size and increase inference speed compared to 8-bit, making it attractive for deployment in resource-constrained environments.


### Types of Quantization

#### Zero-Point Quantization
Let's say you have numbers ranging from -1.0 to 1.0, and you want to represent them with a smaller set of numbers ranging from -127 to 127. To do this, you multiply each original number by 127 (scaling). For example, 0.3 becomes 38 after scaling and rounding. To get back to the original number, you divide by 127, but you'll notice a tiny difference from the original number due to rounding. This process is like trying to fit a wide picture into a smaller frame; you have to trim a bit off the edges, losing some details.

#### Absmax Quantization
Imagine you have a list of numbers, and you find the biggest number among them (ignoring whether it's positive or negative). You use this number to scale all the others so that they fit within a new range, like -127 to 127. For example, if the biggest number is 5.4, you scale all numbers so that 5.4 becomes 127, and the rest adjust accordingly. This method ensures that the most significant number dictates how the rest are scaled, somewhat like adjusting the volume of all instruments in a band to match the loudest one.

## transformers - bitsandbytes Integration

```python
# https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig
# - **`load_in_8bit` and `load_in_4bit`:** These flags enable 8-bit and 4-bit quantization, respectively. They are not enabled by default (`False`) because quantization can affect model performance.
# - **`llm_int8_threshold`:** This is used with 8-bit quantization to handle outliers. Values beyond this threshold are processed in higher precision (fp16) to avoid performance penalties with extreme values. The default threshold is 6, based on the distribution of values in large models.
# - **`llm_int8_skip_modules`:** Lists model components that should not be quantized to 8-bit, useful for models with multiple output heads or specific architectural considerations.
# - **`llm_int8_enable_fp32_cpu_offload`:** Allows parts of the model to run in higher precision (fp32) on the CPU while the rest runs in int8 on the GPU. This is for advanced use cases, such as managing very large models.
# - **`llm_int8_has_fp16_weight`:** For fine-tuning, keeps the main weights in 16-bit to avoid converting them back and forth, improving efficiency.
# - **`bnb_4bit_compute_dtype`:** Sets the computational data type for 4-bit quantization, allowing for different input and computation precisions (e.g., input in fp32 but computation in bf16). The compute dtype is used to change the dtype that will be used during computation. For example, hidden states could be in `float32` but computation can be set to `bf16` for speedups. By default, the compute dtype is set to `float32
# - **`bnb_4bit_quant_type`:** Specifies the data type for 4-bit quantization, with options for FP4 and NF4, affecting how data is quantized and stored.
# - **`bnb_4bit_use_double_quant`:** We also advise users to use the nested quantization technique. This saves more memory at no additional performance - from our empirical observations, this enables fine-tuning llama-13b model on an NVIDIA-T4 16GB with a sequence length of 1024, batch size of 1 and gradient accumulation steps of 4.

from transformers import BitsAndBytesConfig
import torch

bnb_config_4_bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,    #check model.torch_dtype
)

bnb_config_8_bit = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_has_fp16_weight=False,  #check model.torch_dtype
)
```


## Sources:
- https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/
- https://huggingface.co/blog/hf-bitsandbytes-integration

- https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/
- https://huggingface.co/docs/transformers/main/en/quantization#bitsan
- https://huggingface.co/blog/hf-bitsandbytes-integration
- https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf
- https://huggingface.co/docs/transformers/llm_tutorial_optimization
- https://arxiv.org/abs/2208.07339
