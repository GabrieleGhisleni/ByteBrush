

<!-- toc -->

- [AutoModelForCausalLM](#automodelforcausallm)
  * [Params](#params)
  * [Why Use 8-bit Quantization and float16?](#why-use-8-bit-quantization-and-float16)
  * [transformer implementation](#transformer-implementation)
  * [Doubts and inconsistencies](#doubts-and-inconsistencies)
    + [Initialization of QLoRA](#initialization-of-qlora)
    + [unsloth ai](#unsloth-ai)
- [Tokenizer](#tokenizer)
  * [Special Tokens Configuration](#special-tokens-configuration)
  * [Model with no PAD token](#model-with-no-pad-token)
    + [Use [UNK] token](#use-unk-token)
    + [Set `pad_token` to `eos_token`](#set-pad_token-to-eos_token)
      - [Set the Attention Mask for the `eos_token`](#set-the-attention-mask-for-the-eos_token)
    + [Add a new token to the tokenizer](#add-a-new-token-to-the-tokenizer)
    + [`setup_chat` format](#setup_chat-format)
    + [Updating the Generation Configuration](#updating-the-generation-configuration)
  * [transformer implementation with [UNK] token](#transformer-implementation-with-unk-token)
  * [Troubleshooting](#troubleshooting)
    + [Padding Side](#padding-side)
    + [Generation never stops](#generation-never-stops)

<!-- tocstop -->


# AutoRegressive Models

Auto-regressive language models, such as the GPT (Generative Pre-trained Transformer) series, are a type of neural network architecture designed for generating or predicting sequences of data, particularly text. These models are called "auto-regressive" because they generate each element of the sequence by conditioning on the previously generated elements. This approach is fundamentally different from auto-encoding models, which typically generate an output sequence in one go or by conditioning on the entire input sequence.

The core mathematical concept behind auto-regressive models is the factorization of the joint probability distribution of a sequence into conditional distributions. Given a sequence of tokens (words or characters) $X = (x_1, x_2, ..., x_n)$ the joint probability of the sequence can be decomposed as:

$P(X) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,...,x_{n-1})$

This decomposition allows the model to generate a sequence token by token, where each token is generated based on the tokens that have been generated previously.

## Decoder only - Transformer Architecture

Auto-regressive language models like the GPT series primarily use only the decoder part of the Transformer architecture. Unlike the original Transformer model, which consists of an encoder and a decoder for tasks like translation (where the encoder processes the input sequence and the decoder generates the output sequence), GPT and similar models are designed for tasks that generate text based on previous context. 

![decoder-only](https://pbs.twimg.com/media/FsQIDTeaUAA4-3M.jpg)

### Masked Self-Attention
Unlike the full self-attention mechanism that allows each token in the sequence to attend to all other tokens, masked self-attention restricts each token from attending to future tokens in the sequence. This is analogous to providing a form of sequence padding that ensures the model can only use previous and current tokens to predict the next token, thereby preserving the auto-regressive property. For instance, consider the process of translating a sentence where the model has partially translated text. The model can only "see" the portion of the sentence it has already translated (or generated in the case of language modeling), and the future tokens are effectively "masked" or hidden from view. This ensures that during training and generation, the model predictions are made based on the available context without peeking into the future.

iter 1:
input = `['I', 'like', 'to']` masked_value = `['eat', 'pizza', '[END]']`

iter 2:
input = `['I', 'like', 'to', 'eat']` masked_value = `['pizza', '[END]']`

iter 3:
input = `['I', 'like', 'to', 'eat', 'pizza']` masked_value = `['[END]']`

iter 4:
input = `['I', 'like', 'to', 'eat', 'pizza', '[END]']` masked_value = `[]`



This mechanism is essential for training the model to predict the next word in a sequence based on the context of the preceding words.

During the generation phase, the model uses the tokens it has generated so far as context to predict the next token. This process is repeated iteratively. Various strategies can be employed to select the next token, such as sampling from the probability distribution output by the model or picking the most likely token (greedy decoding).

![decoder-only](https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1684227482-3-transformers-explained.png&w=3840&q=75)

### Positional Encoding
Since the model uses only the decoder, which doesn't inherently understand the order of the sequence (because the self-attention mechanism treats all positions equally), positional encodings are added to the input embeddings to provide information about the position of each token in the sequence.

### Layer Composition
The decoder layers in GPT consist of masked self-attention mechanisms followed by position-wise feed-forward networks, with normalization and residual connections. This setup allows the model to generate a token based on the context of the previously generated tokens in an auto-regressive manner.

### Cross-Entropy Loss

The Cross-Entropy Loss is a standard loss function used in classification tasks, including language modeling, where the goal is to predict the next token in a sequence. For a given token, the model outputs a probability distribution over the entire vocabulary, indicating the likelihood of each vocabulary token being the next token. The Cross-Entropy Loss for a single prediction is calculated as follows:

$$L = -\sum_{c=1}^{M} y_{o,c} \log(p_{o,c})$$

where:
- $L$ is the loss for a single prediction.
- $M$ is the number of classes (vocabulary size).
- $y_{o,c}$ is a binary indicator (0 or 1) of whether class label $c$ is the correct classification for observation $o$.
- $p_{o,c}$ is the predicted probability of observation $o$ being of class $c$.

In the context of language modeling, $y_{o,c}$ would be 1 for the actual next token in the sequence and 0 for all other tokens, making the loss a measure of how well the model's predicted probabilities align with the actual next token.

### Handling Padding in Loss Calculation

When training on batches of sequences, it's common to pad shorter sequences to match the length of the longest sequence in the batch. However, these padding tokens do not carry meaningful information and should not contribute to the loss calculation. To handle this, a mask is applied to the loss calculation to ignore the padding tokens.

Suppose we have a batch of sequences with a padding token represented as $PAD$. We can define a mask $M$ for a sequence, where $M_i = 0$ if the $i$-th token is a padding token and $M_i = 1$ otherwise. The modified Cross-Entropy Loss that accounts for padding can then be expressed as:

$$L_{masked} = -\sum_{i=1}^{N} M_i \sum_{c=1}^{M} y_{i,c} \log(p_{i,c})$$

where:
- $N$ is the number of tokens in the sequence (including padding).
- $M_i$ is the mask value for the $i$-th token.
- $y_{i,c}$ and $p_{i,c}$ are defined as before, for the $i$-th token.

This modification ensures that the loss is calculated only for the meaningful tokens in the sequence, effectively ignoring the padding tokens.

### transformer implementation


```python
# - **low_cpu_mem_usage**: This option tries to minimize the amount of memory your computer's CPU uses when loading the model. It aims to keep the memory usage close to the size of the model itself. This is still being tested and might change in the future.
# - **torch_dtype**: This lets you choose the type of numbers (data type) the model uses when it's loaded. Different data types use different amounts of memory and can affect the speed and accuracy of your model. Options include:
#   - `torch.float16` or `torch.bfloat16` or `torch.float`: Pick one of these to set the data type directly.
#   - `"auto"`: The model tries to use the data type specified in its configuration file or, if that's not available, the data type of the model's first weight. This helps in loading the model as it was saved, but doesn't guarantee it reflects the data type used during training.
#   - **device_map**: This tells the model where to run its computations, like on a CPU, a specific GPU, or other devices. You can specify this generally (e.g., all on GPU 0) or in detail (specific parts on specific devices). If you set it to `"auto"`, the system will figure out the best setup for you. From the official doc: `get_kbit_device_map()` https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py. All the options will produce the same result when you don’t have enough GPU memory to accommodate the whole model (which is to fit everything that can on the GPU, then offload weights on the CPU or even on the disk if there is not enough RAM). 
#   When you have more GPU memory available than the model size, here is the difference between each option: 
#     - "auto" and "balanced" evenly split the model on all available GPUs, making it possible for you to use a batch size greater than 1.
#     - "balanced_low_0" evenly splits the model on all GPUs except the first one, and only puts on GPU 0 what does not fit on the others. This option is great when you need to use GPU 0 for some processing of the outputs, like when using the generate function for Transformers models
#     - "sequential" will fit what it can on GPU 0, then move on GPU 1 and so forth (so won’t use the last GPUs if it doesn’t need to).
# - **attn_implementation**: This option lets you choose how the model calculates attention, which is a key part of how it processes inputs. Options include:
#   - `"eager"`: A basic, manually coded way of calculating attention.
#   - `"sdpa"`: Uses a built-in PyTorch function for calculating attention, available if you're using PyTorch version 2.1.1 or newer.
#   - `"flash_attention_2"`: Uses a specific, optimized way of calculating attention from Dao-AILab/flash-attention.
# - **`use_cache`**: If set to `True`, the model keeps track of and returns certain computations that can be reused in future predictions. This can speed up the model but not all models support this feature.
# - **`revision` & `trust_remote_code`:** they are needed to load the architetucture of the models and other config files. Revision points to a specific commit or version while trust_remote_code avoid always say yes when download the files.
# Loading a model in 8-bit precision and specifying `torch_dtype=torch.float16` might seem redundant or confusing at first glance, but they serve related yet distinct purposes in the context of optimizing model performance and memory usage. Let's clarify their roles:


import torch
from transformers import AutoModelForCausalLM
from peft import prepare_model_for_kbit_training
from trl.trainer.utils import get_kbit_device_map

attn_implementation = None
if torch.cuda.get_device_capability()[0] >= 8: 
    !pip install ninja packaging
    !MAX_JOBS=4 pip install flash-attn --no-build-isolation
    attn_implementation = "flash_attention_2"

# model_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
model_id="mistralai/Mistral-7B-Instruct-v0.2"

BIT_PRECISION = "4"
bnb_config = bnb_config_4_bit if BIT_PRECISION == "4" else bnb_config_8_bit

SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=model_id,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16,
    quantization_config=bnb_config,
    device_map='sequential', # get_kbit_device_map() or 'auto'
    revision=None,
    use_cache=False,
    trust_remote_code=True,
    attn_implementation=attn_implementation,
)
```
#### Why Use 8-bit Quantization and float16?

- **Compatibility and Performance**: The 8-bit loading is primarily a memory optimization technique, allowing for the model to be loaded with a significantly reduced memory footprint. However, not all operations during training or inference might be supported or optimized for 8-bit computations on all hardware. By setting the model's `torch_dtype` to float16, you ensure broader compatibility with GPU-accelerated operations, which are well-supported and optimized for float16. This means that while the model's parameters might be stored more compactly, computations during forward and backward passes can still leverage the speed of float16 operations.


## Sources
- https://mvschamanth.medium.com/decoder-only-transformer-model-521ce97e47e2
- https://arxiv.org/pdf/2312.05934.pdf
- https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
- https://github.com/openai/transformer-debugger
