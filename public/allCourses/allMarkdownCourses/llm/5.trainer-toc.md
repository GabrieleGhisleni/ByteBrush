

<!-- toc -->

- [TrainingArguments](#trainingarguments)
  * [transformer implementation](#transformer-implementation)
    + [SFTT Trainer](#sftt-trainer)
    + [Train and save](#train-and-save)

<!-- tocstop -->

# Training 

There are some key concepts related to training optimization and configuration. 
Let's start by commenting the paper from microsoft [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/pdf/2312.05934.pdf)

One important distinction to make is between knowledge that the model has been exposed to before during pre-training as opposed to entirely new facts. Considering the size of modern LLM training sets, they cover a vast amount of information available through web-sourced text. As a result, even in niche domains, the goal of knowledge injection is not necessarily to teach the model entirely new facts but rather to ”refresh” its memory by inducing a bias toward a particular domain.

Making a dataset that only tests knowledge without requiring some thinking is hard. Because of this, a model that's good at reasoning could do well on new, knowledge-heavy tasks by making smart guesses, like in a multiple-choice test. So, when checking how much knowledge large language models (LLMs) have, it's important to also think about how well they reason. This means looking at their performance across different tests, including reasoning, understanding what they read, and their overall language skills.

Fine-tuning involves tweaking a model that has already been trained on a broad task so it performs better on a more specific task or dataset. It's important to understand that there are different ways to do fine-tuning. These methods can be grouped into three categories: supervised, unsupervised, and those based on reinforcement learning (RL). 

- Supervised Fine-Tuning (SFT) involves using labeled examples where each example has a specific input and a desired output. A popular approach within SFT is called instruction tuning, which has proven to be very effective in enhancing the performance of large language models (LLMs). In instruction tuning, the model is given a task described in everyday language and is shown what the correct response looks like. This method has significantly improved models, especially in their ability to perform tasks they haven't seen before and in logical thinking. However, instruction tuning doesn't necessarily equip the model with new knowledge, **which means it's not the ultimate solution for adding new information to a model**.

- Reinforcement Learning (RL) is another technique used in fine-tuning, which adjusts the model's behavior based on strategies inspired by RL. This includes learning from human feedback, optimizing based on direct preferences, and using algorithms that adjust policies to get better outcomes. These methods are particularly effective when combined with instruction tuning. Yet, like instruction tuning, they mainly enhance how the model responds rather than expanding its knowledge base.

- Unsupervised Fine-Tuning doesn't rely on labeled data. Instead, it uses techniques like continual pre-training or unstructured fine-tuning, where the model continues to learn from data without specific guidance on what the correct outputs are. This approach allows the model to keep updating itself without needing explicit examples of right and wrong answers. In this approach, fine-tuning (FT) is seen as an extension of the initial training stage. We begin with a previously saved version of the large language model (LLM) and continue training it to predict the next word or token in a sequence. A key difference from the original training phase is the need for a slower learning pace. This slower pace is crucial to prevent the model from losing previously learned information, a problem known as catastrophic forgetting.

Data augmentation, a proven technique for boosting language models' performance, led to significant enhancements in our study. We discovered a clear link between the use of more paraphrases and increased accuracy in the models. Our tests showed a consistent pattern: the accuracy of all models steadily rose with the number of paraphrases added. This finding strongly indicates that using paraphrases to repeat information helps models better understand and apply new knowledge from a limited amount of data.

Our experiments revealed a notable trend, as seen in one of our figures. After each training cycle, where the model goes through the entire dataset again, there's a big drop in training loss. This aligns with previous findings that language models tend to memorize data during training, which can lead to overfitting. Our theory is that to effectively teach pre-trained language models new information, this information needs to be presented in various forms. This concept is already recognized in the initial training phase of language models and, as our findings suggest, is equally important during the fine-tuning phase.

## Training tricks
### Scheduler
The main idea behind using a scheduler is to start with a higher learning rate to make rapid progress in reducing the loss, and then decrease the learning rate as training progresses to allow the model to converge more finely and avoid overshooting the minimum of the loss function. This approach can help in achieving faster convergence and potentially better generalization.

There are several types of learning rate schedulers, each with its own strategy for adjusting the learning rate during training. Here are some of the most commonly used schedulers:

![scheduler](https://pbs.twimg.com/media/FeQmxyQX0AAx63h?format=jpg&name=4096x4096)

1. **Fixed Learning Rate**: The simplest approach, where the learning rate remains constant throughout the training process. While not technically a scheduler, it serves as a baseline for comparison.

2. **Step Decay**: The learning rate is reduced by a factor after a specified number of epochs. For example, the learning rate might be reduced by a factor of $\gamma$ every $N$ epochs. The update rule can be expressed as:

   $$ \eta_t = \eta_0 \cdot \gamma^{\left\lfloor \frac{t}{N} \right\rfloor} $$

   where $\eta_t$ is the learning rate at epoch $t$, $\eta_0$ is the initial learning rate, $\gamma$ is the decay factor, and $N$ is the step size (number of epochs after which the learning rate is decayed).

3. **Exponential Decay**: The learning rate decreases exponentially, following the formula:

   $$ \eta_t = \eta_0 \cdot e^{-kt} $$

   where $k$ is a hyperparameter that controls the rate of decay.

4. **Cosine Annealing**: This scheduler adjusts the learning rate following a cosine curve between an upper and lower bound over a predefined number of epochs (a cycle), potentially followed by a restart. The learning rate at epoch $t$ is given by:

   $$ \eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t \pi}{T}\right)\right)$$

   where $\eta_{\min}$ and $\eta_{\max}$ are the minimum and maximum learning rates, respectively, and $T$ is the total number of epochs in a cycle.

5. **Learning Rate Warmup**: A strategy often used in conjunction with other schedulers, where the learning rate starts from a very small value and gradually increases to a target learning rate over a few initial epochs or steps. This can help in stabilizing the training in the early phases.

6. **OneCycle Policy**: This approach aims to train models faster and with better accuracy by starting with a learning rate warmup, followed by a high learning rate phase for rapid progress, and finally reducing the learning rate significantly to allow fine convergence. The learning rate typically follows a cosine annealing schedule within a single cycle.

### Gradient Accumulation Steps
are a practical solution for training models with large batch sizes on hardware with limited memory capacity. The idea is to divide a large batch into smaller mini-batches and accumulate the gradients over these mini-batches before performing a single update to the model parameters. Mathematically, if you have a batch size that you wish to simulate, $B$, and your hardware can only accommodate a batch size of $b$, you can perform gradient accumulation over $n = B / b$ steps. The gradients $\nabla \theta$ computed at each step are accumulated (summed) before performing the update:

$$\Delta \theta = \frac{1}{n} \sum_{i=1}^{n} \nabla \theta_i$$

This approach effectively simulates training with a larger batch size, improving the stability and quality of the updates without increasing memory requirements.

### Weight Decay
is a regularization technique used to prevent overfitting by penalizing large weights in the model. It works by adding a fraction of the weight values to the loss function, encouraging the model to maintain smaller weights. If $\lambda$ is the weight decay coefficient and $L$ is the original loss function, the modified loss function $L'$ with weight decay is:

$$L' = L + \frac{\lambda}{2} \sum_{i} \theta_i^2$$

In the AdamW optimizer, weight decay is applied differently from traditional L2 regularization by decoupling the weight decay from the optimization steps. This approach is more effective for adaptive gradient algorithms like Adam.

- In many applications, weight decay values in the range of **1e-4** to **1e-2** are common. For example, a weight decay of **1e-4** or **0.0001** is a typical default value in many deep learning frameworks when training convolutional neural networks (CNNs) for image classification tasks.
- For training large models such as those used in NLP, including the Transformer models like BERT or GPT, a smaller weight decay, such as **1e-5** or **1e-6**, might be used. This is because these models are often trained on very large datasets, and too much regularization can hinder the model's ability to fit the training data well.

### Max Gradient Norm
is a parameter used in conjunction with gradient clipping, a technique to prevent exploding gradients, which can destabilize the training process. Gradient clipping involves scaling the gradients of the model's parameters if the norm of the gradient exceeds a certain threshold, $\tau$ (max_grad_norm). For a gradient vector $g$, if its norm $\|g\| > \tau$, it is scaled down to a smaller vector $g'$ such that its norm is equal to $\tau$:

$$g' = \frac{\tau}{\|g\|} g$$

This ensures that the updates to the model parameters remain small and manageable, preventing large updates that could lead to instability in training.

- For many tasks, setting the max gradient norm to **1.0** is a good starting point. This is particularly common in natural language processing (NLP) tasks, where gradients can sometimes grow large due to the long sequences processed.
- In some cases, especially when training is particularly unstable, or the model is very deep, a higher value such as **5.0** or **10.0** might be used to allow for larger updates while still preventing the most extreme cases of exploding gradients.

### Gradient Checkpointing
is a technique to reduce memory usage during training at the cost of additional computation. In standard backpropagation, intermediate activations are stored in memory for use during the gradient computation in the backward pass. Gradient checkpointing reduces memory usage by discarding these intermediate activations and recomputing them as needed during the backward pass. While this approach saves memory, allowing for larger models or batch sizes to be trained, it increases the computational overhead due to the extra recomputation.

### Flash attention 
Before diving into Flash Attention, let's briefly review the standard attention mechanism. In the context of Transformers, the most commonly used form of attention is self-attention, which allows inputs to interact with each other ("self") and find out who they should pay more attention to. The self-attention mechanism computes three vectors for each input: Query ($Q$), Key ($K$), and Value ($V$). The output of the attention layer is a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key:

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

Here, $d_k$ is the dimensionality of the key vectors, and the softmax function is applied to ensure that the weights sum to 1.

The computation of $QK^T$ involves a matrix multiplication that has a quadratic complexity with respect to the sequence length, making it computationally expensive for long sequences. This poses challenges in terms of both memory usage and computational efficiency, limiting the applicability of Transformers to tasks with very long sequences or requiring significant computational resources.

Flash Attention emerges as an optimization technique aimed at addressing the computational inefficiencies of the standard attention mechanism. While detailed technical specifics of Flash Attention might evolve, the core idea revolves around optimizing the computation of the attention mechanism to reduce its complexity, especially for long sequences.

The key innovations of Flash Attention could include:
- **Efficient Memory Access Patterns**: By reorganizing the computation and memory access patterns, Flash Attention can significantly reduce the time and memory overhead associated with computing attention for long sequences.
- **Reducing Redundant Computations**: Flash Attention may employ strategies to identify and eliminate redundant computations within the attention mechanism, focusing computational resources only where necessary.
- **Parallelization**: Leveraging advanced parallelization techniques to distribute the computation of attention across multiple processing units can lead to substantial speedups.


### transformer implementation

```python
# - **`gradient_accumulation_steps`**: (int, defaults to 1) — Number of updates steps to accumulate the gradients for, before performing a backward/update pass.  This parameter allows you to accumulate gradients over multiple steps before performing a backward/update pass. This is particularly useful when you're limited by GPU memory and can't fit a large batch size. By accumulating gradients, you effectively simulate a larger batch size without the need for more memory. When to use: Use this when you're dealing with large models or datasets and facing memory constraints. It helps in training with larger effective batch sizes.
# - **`weight_decay`**: (float, defaults to 0) — The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer. Applies weight decay regularization, which helps in preventing overfitting by penalizing large weights. In the AdamW optimizer, weight decay is applied to all parameters except biases and LayerNorm weights, as these are typically not regularized. Use this to improve generalization of your model. It's especially useful when you suspect your model is overfitting.
# - **`max_grad_norm`:** (float, optional, defaults to 1.0) — Maximum gradient norm (for gradient clipping). This is for gradient clipping. It limits the norm of the gradient vector to prevent exploding gradients, which can destabilize training. Useful in training deep neural networks where exploding gradients can be an issue, helping to stabilize training.
# - **`gradient_checkpointing`:** (bool, optional, defaults to False) — If True, use gradient checkpointing to save memory at the expense of slower backward pass. Saves memory by trading compute for memory usage. It recomputes some forward pass elements during the backward pass instead of storing them. In very deep models where memory is a constraint. It slows down training but allows for training larger models or using larger batch sizes.
#   - **`gradient_checkpointing_kwargs`:** (dict, optional, defaults to None) — Key word arguments to be passed to the gradient_checkpointing_enable method.
# - **`torch_compile`:** (bool, optional, defaults to False) — Whether or not to compile the model using PyTorch 2.0 torch.compile. Also keep in mind that torch.compile() won't give you major speedups on a T4 GPU, it's much more optimized for Ampere+ architectures https://github.com/pytorch/pytorch/issues/107960
# - **`save_safetensors`:** (bool, optional, defaults to True) — Use safetensors saving and loading for state dicts instead of default torch.load and torch.save. Uses a safer method for saving and loading state dictionaries, which can be beneficial in avoiding potential data corruption. It's a good practice to leave this enabled unless you have specific reasons not to.
# - **`jit_mode_eval`:** (bool, optional, defaults to False) — Whether or not to use PyTorch jit trace for inference.
# - **`bf16`:** (bool, optional, defaults to False) — Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change. Enables mixed precision training using bfloat16, which can significantly speed up training and reduce memory usage on supported hardware. When you have compatible hardware (Ampere or newer NVIDIA GPUs, certain CPUs, or Ascend NPUs) and are looking to improve training efficiency.
# - **`fp16`:** (bool, optional, defaults to False) — Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.
# fp16_opt_level (str, optional, defaults to ‘O1’) — For fp16 training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details on the Apex documentation. Similar to `bf16`, but uses float16 for mixed precision training. It's widely supported on many NVIDIA GPUs and can provide similar benefits. When you're looking to speed up training and reduce memory footprint, especially if you're on hardware that doesn't support `bf16`.
# - **`tf32`:** (bool, optional) — Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends on PyTorch’s version default of torch.backends.cuda.matmul.allow_tf32. For more details please refer to the TF32 documentation. This is an experimental API and it may change. Enables TensorFlow 32-bit floating-point format on compatible NVIDIA GPUs, which can offer a balance between performance and precision. On Ampere or newer NVIDIA GPUs when you want to potentially improve performance without significantly impacting precision.
# - **`lr_scheduler_type`**: (str or SchedulerType, optional, defaults to "linear") — The scheduler type to use. See the documentation of SchedulerType for all possible values.
#   - **`lr_scheduler_kwargs`**: (‘dict’, optional, defaults to {}) — The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.
#   - **`warmup_ratio`**: (float, optional, defaults to 0.0) — Ratio of total training steps used for a linear warmup from 0 to learning_rate.
#   - **`warmup_steps`** (int, optional, defaults to 0) — Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.
# - **`learning_rate`**: (float, defaults to 5e-5) — The initial learning rate for AdamW optimizer.
#   - **adam_beta1** (float, optional, defaults to 0.9) — The beta1 hyperparameter for the AdamW optimizer.
#   - **adam_beta2** (float, optional, defaults to 0.999) — The beta2 hyperparameter for the AdamW optimizer.
#   - **adam_epsilon** (float, optional, defaults to 1e-8) — The epsilon hyperparameter for the AdamW optimizer.
# - **`num_train_epochs`**: (float, optional, defaults to 3.0) — Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).
# - **`max_steps`**: (int, optional, defaults to -1) — If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.
# - **`evaluation_strategy`:** The evaluation strategy to adopt during training. Possible values are:
#   - "no": No evaluation is done during training.
#   - "steps": Evaluation is done (and logged) every eval_steps.
#   - "epoch": Evaluation is done at the end of each epoch.
# - **`per_device_train_batch_size`:** (int, default 8) — The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.
# - **`logging_strategy`:** (str or IntervalStrategy, optional, defaults to "steps") — The logging strategy to adopt during training. Possible values are:
#   - "no": No logging is done during training.
#   - "epoch": Logging is done at the end of each epoch.
#   - "steps": Logging is done every logging_steps
# - **`logging_steps`:** (int or float, optional, defaults to 500) — Number of update steps between two logs if logging_strategy="steps". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.
# - **`save_strategy`:** (str or IntervalStrategy, optional, defaults to "steps") — The checkpoint save strategy to adopt during training. Possible values are:
# "no": No save is done during training.
# "epoch": Save is done at the end of each epoch.
# "steps": Save is done every save_steps.
# - **`save_steps`:** (int or float, optional, defaults to 500) — Number of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.
# - **`dataloader_num_workers`:** (int, optional, defaults to 0) — Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process.
# - **`run_name`:** (str, optional) — A descriptor for the run. Typically used for wandb and mlflow logging.
# - **`optim`:** (str or training_args.OptimizerNames, optional, defaults to "adamw_torch") — The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.
# - **`optim_args`:** (str, optional) — Optional arguments that are supplied to AnyPrecisionAdamW.
# - **`group_by_length`:** (bool, optional, defaults to False) — Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding.
# - **`report_to`:** (str or List[str], optional, defaults to "all") — The list of integrations to report the results and logs to. Supported platforms are "azure_ml", "clearml", "codecarbon", "comet_ml", "dagshub", "dvclive", "flyte", "mlflow", "neptune", "tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no integrations.


  
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=output_model,
    # max_steps=-1,
    logging_steps=10,
    num_train_epochs=2,
    save_strategy="epoch",
    per_device_train_batch_size=4,
    save_safetensors=True,

    torch_compile=False,
    gradient_accumulation_steps=1,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs=dict(use_reentrant=False),
    fp16=False,
    bf16=False,
    tf32=False,

    max_grad_norm=0.3,
    learning_rate=5e-5,
    optim="adamw_8bit", # "adamw_8bit", "adamw_torch_fused"
    lr_scheduler_type="constant",
    warmup_ratio=0.03,
    
    seed=42,

    run_name=output_model
    group_by_length=False,
    report_to="wandb",
    push_to_hub=False,
)
```

## Sources
- https://arxiv.org/abs/2310.07521
- https://arxiv.org/abs/2307.09288
- https://arxiv.org/abs/2205.10770
- https://arxiv.org/pdf/2312.05934.pdf
